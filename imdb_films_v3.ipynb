{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "imdb_films_v3",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNPbmvCFKCfg"
      },
      "source": [
        "# Решение задачи определения эмоциональной окраски отзывов к фильмам с IMDB с помощью нескольких способов\n",
        "\n",
        "1) Сначала данные загружаются из Keras, немного преобразуются и тренируется модель\n",
        "\n",
        "2) Потом загружаются предобученные векторные представления, а данные загружаются архивом и вручную разбиваются на папки. Модель тренируется с использованием предобученных представлений\n",
        "\n",
        "3) Затем модель тренируется на тех же данных, разбитых на папки, но уже без \n",
        "предварительно обученных представлений"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpBiduAvLdUe"
      },
      "source": [
        "Скачивание данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIDJlHEYHjle"
      },
      "source": [
        "from keras.datasets import imdb\n",
        "\n",
        "\n",
        "# всего слов 10_000\n",
        "max_words = 10_000\n",
        "# из каждого отзыва будем брать только первые 20 слов\n",
        "maxlength = 20\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_words)\n",
        "x_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obPKZde3OOPm"
      },
      "source": [
        "Предобработка данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5I46N23GOGY4"
      },
      "source": [
        "from keras import preprocessing\n",
        "\n",
        "# преобразование массива целых чисел в двумерный тензор формы (образцы, максимальная длина)\n",
        "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlength)\n",
        "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlength)\n",
        "x_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AEWJWjGLfBL"
      },
      "source": [
        "Создание модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_hV9QN_LgVa"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "# Определение максимальной длины входа для слоя Embedding в \n",
        "# целях последующего уменьшения размерности.\n",
        "# После слоя Embedding активация имеет форму (образцы, максимальная_длина, 8)\n",
        "# первый аргумент - размер словаря (количество слов)\n",
        "# второй аргумент - одна из размерностей выходного тензора (снижение размерности)\n",
        "# eсли после Embedding есть Flatten или Dense, то необходимо указывать input_length\n",
        "model.add(Embedding(10_000, 8, input_length=maxlength))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deujCMrjNYiC"
      },
      "source": [
        "Тренировка модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0Gv0J50M56r"
      },
      "source": [
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train, \n",
        "    batch_size=32,\n",
        "    validation_split=0.2, \n",
        "    epochs = 30\n",
        ")\n",
        "history = history.history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lrn9fzp8RyTW"
      },
      "source": [
        "Функции для рисование графиков потерь и точности"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbyjcSiNPat1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def draw_loss(history):\n",
        "    loss_values = history[\"loss\"]\n",
        "\n",
        "    epochs = range(1, len(history['loss']) + 1)\n",
        "    plt.plot(epochs, loss_values, 'b', label='Training loss')\n",
        "    plt.title('Training loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Value')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    plt.clf()\n",
        "\n",
        "def draw_acc(history):\n",
        "    acc_values = history[\"accuracy\"]\n",
        "\n",
        "    epochs = range(1, len(history['accuracy']) + 1)\n",
        "    plt.plot(epochs, acc_values, 'r', label='Training accuracy')\n",
        "    plt.title('Training accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Value')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "draw_loss(history)\n",
        "draw_acc(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnaK_RxHq5jR"
      },
      "source": [
        "# Теперь будем использовать предарительно обученное векторное представление слов, но данные IMDB загрузим не из Keras, а архивом и распределим вручную"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-L86EqxrWgM"
      },
      "source": [
        "import os\n",
        "\n",
        "data_dir = os.path.join('/content/drive/My Drive/StudyingKeras/IMDB/data/aclImdb.zip (Unzipped Files)/aclImdb/train')\n",
        "\n",
        "# массив меток\n",
        "labels = []\n",
        "# массив строк - одна строка на отзыв\n",
        "texts = []\n",
        "\n",
        "# метки двух типов: положительные и отрицательные\n",
        "for label_type in ['neg', 'pos']:\n",
        "  # по-очереди переходи между папками\n",
        "  dir_name = os.path.join(data_dir, label_type)\n",
        "  print(f\"Processing file in {dir_name}\")\n",
        "  print(dir_name)\n",
        "  for fname in os.listdir(dir_name):\n",
        "    if fname[-4:] == '.txt':\n",
        "      with open(os.path.join(dir_name, fname)) as f:\n",
        "        texts.append(f.read())\n",
        "      if label_type == 'neg':\n",
        "        labels.append(0)\n",
        "      elif label_type == 'pos':\n",
        "        labels.append(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOu-4ps-uAfX"
      },
      "source": [
        "Токенизация текста из исходного набора данных IMDB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrYy_bGUuEWB"
      },
      "source": [
        "# поскольку мы используем предварительно обученные векторные представления, они не \n",
        "# всегда идеально подходят для любой задачи\n",
        "# стоит ограничить тренировочные данные, чтобы снизить объем вариантов фраз\n",
        "# возьмем первые 200 образцов\n",
        "\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# сто положительных и сто отрицательных отзывов\n",
        "maxlen = 100\n",
        "# обучение на выборке из 200 образцов\n",
        "training_samples = 200\n",
        "# проверка на выборке из 10 000 образцов\n",
        "validation_samples = 10_000\n",
        "# рассматриаем только 10 000 самых частых слов\n",
        "max_words = 10_000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(f\"Found {len(word_index)} unique sequences.\")\n",
        "\n",
        "# преобразование массива целых чисел в двумерный тензор формы (образцы, максимальная длина)\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "labels = np.asarray(labels)\n",
        "print(f'Shape of data tensor is {data.shape}')\n",
        "print(f'Shape of labels tensor is {labels.shape}')\n",
        "\n",
        "# перед тем как разбить данные на тренировочную и тестовую выборки, их \n",
        "# необходимо перемешать, так как изначально они упорядочены\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "# выборки по 200 образцов\n",
        "x_train = data[:training_samples]\n",
        "y_train = labels[:training_samples]\n",
        "# на валидацию данные берем из общих тренировочных\n",
        "x_val = data[training_samples : training_samples + validation_samples]\n",
        "y_val = labels[training_samples : training_samples + validation_samples]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da3LB6v4Rc6r"
      },
      "source": [
        "Разделение данных их файла на тренировочные и тестовые занимает слишком много времени. Они буду сохранены в файл для последующей быстрой работы с ними."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDvmW1S6RMz1"
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open('/content/drive/My Drive/StudyingKeras/IMDB/pickled/train/x_train', 'wb') as f:\n",
        "  pickle.dump(x_train, f)\n",
        "with open('/content/drive/My Drive/StudyingKeras/IMDB/pickled/train/y_train', 'wb') as f:\n",
        "  pickle.dump(y_train, f)\n",
        "with open('/content/drive/My Drive/StudyingKeras/IMDB/pickled/train/x_val', 'wb') as f:\n",
        "  pickle.dump(x_val, f)\n",
        "with open('/content/drive/My Drive/StudyingKeras/IMDB/pickled/train/y_val', 'wb') as f:\n",
        "  pickle.dump(y_val, f)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyGr_AW0w-xW"
      },
      "source": [
        "Обработка файла с векторными представлениями слов GloVe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doeTF1L3xCYw",
        "outputId": "9828b7db-a6d9-4965-aa45-1f373653ca9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "glove_dir = '/content/drive/My Drive/StudyingKeras/IMDB/GloVe weights/glove.6B.zip (Unzipped Files)'\n",
        "embedding_index = {}\n",
        "\n",
        "with open(os.path.join(glove_dir, 'glove.6B.100d.txt')) as f:\n",
        "  for line in f:\n",
        "    values = line.split()\n",
        "    # первое значение - само слово\n",
        "    word = values[0]\n",
        "    # остальные значения - коэффициента для построение вектора\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    # все помещаем в наш словарь\n",
        "    embedding_index[word] = coefs\n",
        "\n",
        "print(f\"Found {len(embedding_index)} pre-trained word vectors\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 pre-trained word vectors\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9S9tErUyV7Y"
      },
      "source": [
        "Создание матрицы векторных представлений слов GloVe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WBDdgPKyZIL"
      },
      "source": [
        "# это должна быть матрица формы (число слов, размерность представления)\n",
        "# каждый элемент матрицы содержит вектор такой же размерности, что и вектор\n",
        "# i-ого слова в созданном словаре embedding_index\n",
        "\n",
        "embedding_dim = 100\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in  word_index.items():\n",
        "  if i < max_words:\n",
        "    embedding_vector = embedding_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Z29vHftzTc1"
      },
      "source": [
        "# Создание модели с предварительно обученным векторным представлением слов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmnIKbyNzVVr"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense, Embedding\n",
        "\n",
        "pretrained_model = Sequential()\n",
        "pretrained_model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "pretrained_model.add(Flatten())\n",
        "pretrained_model.add(Dense(32, activation='relu'))\n",
        "pretrained_model.add(Dense(1, activation='sigmoid'))\n",
        "pretrained_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eafMJN-rzxEx"
      },
      "source": [
        "Загрузка предварительно обученных векторных представлений слов в слой Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JFIrU2D0EE4"
      },
      "source": [
        "# загружаем веса в слой (новый метод)\n",
        "pretrained_model.layers[0].set_weights([embedding_matrix])\n",
        "# замораживаем слой, чтобы веса не изменялись\n",
        "pretrained_model.layers[0].trainable = False\n",
        "pretrained_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1nlvHBT0hcp"
      },
      "source": [
        "Компиляция и обучение модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eUd9lfo0joC"
      },
      "source": [
        "pretrained_model.compile(\n",
        "    optimizer='rmsprop',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "pretrained_history = pretrained_model.fit(\n",
        "    x_train,\n",
        "    y_train, \n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_data=(x_val, y_val)\n",
        ").history\n",
        "pretrained_model.save('/content/drive/My Drive/StudyingKeras/IMDB/weights/pre_trained_glove_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLO2Go4t1S-s"
      },
      "source": [
        "Создание графиков обучения"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQXsZwWq1Yaz"
      },
      "source": [
        "draw_loss(pretrained_history)\n",
        "draw_acc(pretrained_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJVcoj3y46PW"
      },
      "source": [
        "# Создание модели без предварительно обученного векторного представления слов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blszb5N05TSu"
      },
      "source": [
        "raw_model = Sequential()\n",
        "# сколько слов максимум, глубина вектора слова, длина входных слов\n",
        "raw_model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "raw_model.add(Flatten())\n",
        "raw_model.add(Dense(32, activation='relu'))\n",
        "raw_model.add(Dense(1, activation='sigmoid'))\n",
        "raw_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k16WuvPH6VWY"
      },
      "source": [
        "Компиляция и обучение модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAEYL94F6dOc"
      },
      "source": [
        "raw_model.compile(\n",
        "    optimizer='rmsprop', \n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "raw_history = raw_model.fit(\n",
        "    x_train, \n",
        "    y_train,\n",
        "    batch_size=32,\n",
        "    epochs=10,\n",
        "    validation_data=(x_val, y_val)\n",
        ").history\n",
        "raw_model.save('/content/drive/My Drive/StudyingKeras/IMDB/weights/raw_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7Lkkyt86xaA"
      },
      "source": [
        "Создание графиков обучения"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHT1AZEj60rP"
      },
      "source": [
        "draw_loss(raw_history)\n",
        "draw_acc(raw_history)\n",
        "\n",
        "# мы видим, что в данном случае точность на проверке хуже\n",
        "# это значит, что при небольшом объеме выборки (200 слов),\n",
        "# предварительно обученное представление выигрывает у вновь\n",
        "# обученного\n",
        "# если же увеличить размер выборки, то ситуация резко изменится \n",
        "# на противоположную"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnidz9-qST11"
      },
      "source": [
        "Токенизация контрольных данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luLtmOaCSXAw",
        "outputId": "3f3b9e2b-8e5d-4e53-badd-5029c63975b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "data_dir = os.path.join('/content/drive/My Drive/StudyingKeras/IMDB/data/aclImdb.zip (Unzipped Files)/aclImdb/test')\n",
        "\n",
        "# массив меток\n",
        "labels = []\n",
        "# массив строк - одна строка на отзыв\n",
        "texts = []\n",
        "\n",
        "# метки двух типов: положительные и отрицательные\n",
        "for label_type in ['neg', 'pos']:\n",
        "  # по-очереди переходи между папками\n",
        "  dir_name = os.path.join(data_dir, label_type)\n",
        "  print(dir_name)\n",
        "  for fname in os.listdir(dir_name):\n",
        "    if fname[-4:] == '.txt':\n",
        "      with open(os.path.join(dir_name, fname)) as f:\n",
        "        texts.append(f.read())\n",
        "      if label_type == 'neg':\n",
        "        labels.append(0)\n",
        "      elif label_type == 'pos':\n",
        "        labels.append(1)\n",
        "    \n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "x_test = pad_sequences(sequences, maxlen=maxlen)\n",
        "y_test = np.asarray(labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/StudyingKeras/IMDB/data/aclImdb.zip (Unzipped Files)/aclImdb/test/neg\n",
            "/content/drive/My Drive/StudyingKeras/IMDB/data/aclImdb.zip (Unzipped Files)/aclImdb/test/pos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKk0rx431q-k"
      },
      "source": [
        "Предыдущий шаг занимает очень много времени, так что сохраним полученные переменные в файлы"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0o4tLYp1lZ_"
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open('/content/drive/My Drive/StudyingKeras/IMDB/pickled/test/x_test', 'wb') as f:\n",
        "  pickle.dump(x_test, f)\n",
        "with open('/content/drive/My Drive/StudyingKeras/IMDB/pickled/test/y_test', 'wb') as f:\n",
        "  pickle.dump(y_test, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLtkQ_jgGqA1"
      },
      "source": [
        "Загрузка токенизированных данных из pickle файлов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVV2FcUvGzZy"
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "maxlen = 100\n",
        "max_words = 10_000\n",
        "embedding_dim = 100\n",
        "\n",
        "with open('/content/drive/My Drive/StudyingKeras/IMDB/pickled/test/x_test', 'rb') as f:\n",
        "  x_test = pickle.load(f)\n",
        "with open('/content/drive/My Drive/StudyingKeras/IMDB/pickled/test/y_test', 'rb') as f:\n",
        "  y_test = pickle.load(f)\n",
        "with open('/content/drive/My Drive/StudyingKeras/IMDB/pickled/train/x_train', 'rb') as f:\n",
        "  x_train = pickle.load(f)\n",
        "with open('/content/drive/My Drive/StudyingKeras/IMDB/pickled/train/y_train', 'rb') as f:\n",
        "  y_train = pickle.load(f)\n",
        "with open('/content/drive/My Drive/StudyingKeras/IMDB/pickled/train/x_val', 'rb') as f:\n",
        "  x_val = pickle.load(f)\n",
        "with open('/content/drive/My Drive/StudyingKeras/IMDB/pickled/train/y_val', 'rb') as f:\n",
        "  y_val = pickle.load(f)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aPAyvleWULG"
      },
      "source": [
        "# Оценка моделей"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1r9jtPTZynA"
      },
      "source": [
        "Оценка модели c предобучением на вручную собранном контрольном наборе данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_nCGTX-Z6qM"
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "pretrained_model = load_model('/content/drive/My Drive/StudyingKeras/IMDB/weights/pre_trained_glove_model.h5')\n",
        "pretrained_model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "carudnQaLr7L"
      },
      "source": [
        "Оценка заново натренированной модели на вручную собранном контрольном наборе данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Q_AZWJ0Lqab"
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "raw_model = load_model('/content/drive/My Drive/StudyingKeras/IMDB/weights/raw_model.h5')\n",
        "raw_model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}