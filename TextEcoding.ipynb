{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextEcoding",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lzy0k-22O3ZU"
      },
      "source": [
        "# НАЧАЛО ГЛАВЫ 6: ГЛУБОКОЕ ОБУЧЕНИЕ ДЛЯ ТЕКСТА И ПОСЛЕДОВАТЕЛЬНОСТЕЙ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJpW6O3J5p1p"
      },
      "source": [
        "# One-hot encoding (прямое кодирование)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPi4Yza2PASo"
      },
      "source": [
        "Прямое кодирование на уровне слов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vt55i0h6OvKK"
      },
      "source": [
        "# прямое кодирование (one-hot encoding) делается в общем так:\n",
        "# - каждому слову текста присваивается номер i\n",
        "# - создается вектор размером с длину алфавита, из которого состоят эти слова\n",
        "# - i-ый элемент этого вектора = 1, остальные = 0\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# один элемент = один образец\n",
        "samples = ['The cat sat on the mat', 'The dog ate my homework']\n",
        "\n",
        "# словарь token - его индекс\n",
        "token_index = {}\n",
        "i = 0\n",
        "for sample in samples:\n",
        "  for word in sample.split():\n",
        "    if word not in token_index:\n",
        "      token_index[word] = i\n",
        "      i += 1\n",
        "\n",
        "# максимальное шифруемое количество слов\n",
        "max_length = 10\n",
        "\n",
        "# трехмерный тензор, где каждый двумерный тензор = слово\n",
        "# .values() = [1,2,3,4,5,6,7,8,9,10]\n",
        "results = np.zeros(shape=(len(samples), max_length, max(token_index.values()) + 1))\n",
        "\n",
        "for i, sample in enumerate(samples):\n",
        "  for j, word in enumerate(sample.split()):\n",
        "    # получение индекса слова в предложении\n",
        "    index = token_index.get(word)\n",
        "    results[i, j, index] = 1\n",
        "\n",
        "for el in results:\n",
        "  print(el)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XzRg9quUGHX"
      },
      "source": [
        "Прямое кодирование на уровне символов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RC7H3IV_UIh7"
      },
      "source": [
        "import string\n",
        "\n",
        "samples = ['The cat sat on the mat', 'The dog ate my homework']\n",
        "# все существующие символы ASCII\n",
        "characters = string.printable\n",
        "token_index = {}\n",
        "for j, character in enumerate(characters):\n",
        "  token_index[character] = j\n",
        "\n",
        "max_length = 50\n",
        "results = np.zeros((len(samples), max_length, max(token_index.values()) + 1))\n",
        "\n",
        "for i, sample in enumerate(samples):\n",
        "  for j, character in enumerate(sample):\n",
        "    index = token_index.get(character)\n",
        "    results[i, j, index] = 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0O7B_otnYeVb"
      },
      "source": [
        "Прямое кодирование слов с помощью Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLvTGITqY7lj"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "samples = ['The cat sat on the mat', 'The dog ate my homework', 'zhopa']\n",
        "\n",
        "# такой токенизатор сможет закодировать 1000 слов\n",
        "tokenizer = Tokenizer(num_words=1000)\n",
        "# создание индексов всех слов\n",
        "tokenizer.fit_on_texts(samples)\n",
        "# преобразование слов в списке целочисленных индексов\n",
        "tokenizer.texts_to_sequences(samples)\n",
        "# получение прямого кодирование one-hot\n",
        "# матрица формы (len(samples), num_words)\n",
        "# каждый i-ый элемент - один образец (предложение или слово - не важно)\n",
        "one_hot_code = tokenizer.texts_to_matrix(samples, mode='binary')\n",
        "print(f'Found {tokenizer.word_index} unique tokens')\n",
        "\n",
        "for i, sample in enumerate(samples):\n",
        "  print(f'Encoded {sample} is: \\n {one_hot_code[i]}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pPP8Uzax38T"
      },
      "source": [
        "Прямое кодирование на уровне слов у использованием хэширования"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvKE477Nx2ZV"
      },
      "source": [
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "\n",
        "# слова будут сохранятся как тензоры длины 1000\n",
        "# чем ближе количество слов к 1000, тем больше хэш-коллизий - когда два\n",
        "# слова получают одинаковых хэш-код и сетка не может их различить\n",
        "dims = 1000\n",
        "max_length = 10\n",
        "\n",
        "results = np.zeros((len(samples), max_length, dims))\n",
        "for i, sample in enumerate(samples):\n",
        "  for j, word in enumerate(sample.split()):\n",
        "    # встроенная функция hash возвращает хэш-код слова или символа\n",
        "    index = abs(hash(word)) % dims\n",
        "    print(f'index is {index}')\n",
        "    results[i, j, index] = 1\n",
        "\n",
        "for i, sample in enumerate(samples):\n",
        "  for j, word in enumerate(sample.split()):\n",
        "    print(f'Encoded {word} is {results[i][j]}')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZM5T5Ou5z7Y"
      },
      "source": [
        "# Векторное представление слов (часть 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WUkCE3e53ul"
      },
      "source": [
        "# прямое кодирование слов дает многоразмерные разреженные тензоры\n",
        "# есть векторное представление слов\n",
        "# векторное представление конструируется на основе даннх, имеет меньшую размерность\n",
        "# синонимы представляются схожими векторами\n",
        "# расстояние между векторами в n-мерном пространстве показывает схожесть их семантики\n",
        "# пусть на двумерной плоскости есть две точни (собака) (кошка), тогда вектор \n",
        "# (собака)-(кошка) имеет логический смысл - переход от псовых к кошачим\n",
        "# универсального векторного пространства для всего человеческого языка НЕТ\n",
        "# для каждой новой задачи нужно обучать новое пространство"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWcyr_L4-prl"
      },
      "source": [
        "from keras.layers import Embedding\n",
        "\n",
        "# Embedding принимает минимум два аргумента: количество токенов и размерность\n",
        "# 1000 слов, каждое = вектор длины 64\n",
        "# Embedding работает как словарь. Он принимает на вход целое число,\n",
        "# ищет его во внутреннем словаре по ключам. Возвращает значение - тензор\n",
        "# ПРИНИМАЕТ тензор (образцы, длина последовательности)\n",
        "# ВОЗВРАЩАЕТ тензор (образцы, длина последовательности, размерность_векторного представления)\n",
        "embedding_layer = Embedding(1000, 64)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}