{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "one_dim_nn_plus_RNN",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Wz9RyI8IfAM"
      },
      "source": [
        "Итак, ОСС хорошо подходят для определения эмоциональной окраски текста, но плохо - для предсказаний температуры.\n",
        "\n",
        "В данном ноутбуке представлено универсальное решение.\n",
        "\n",
        "ОСС обрабатывает ооочень длинну. последовательность, сжимая ее размерность, а затем полученные данные помещаются в RNN, которая работает с большей точностью"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y80kU26YI3Ot"
      },
      "source": [
        "Предсказание температуры с помощью объединения ОСС и RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCBVxzk8IYXk"
      },
      "source": [
        "# все данные идентичны ноутбуку RNN_jena\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data_path = '/content/drive/My Drive/StudyingKeras/RNN/jena_climate_data_2009_20016/jena_climate_2009_2016.csv'\n",
        "\n",
        "# Этот код выведет 420 551 строку с данными - каждая строка соответствует\n",
        "# одному замеру и содержит дату замера и 14 значений разных параметров\n",
        "data = pd.read_csv(data_path)\n",
        "\n",
        "\n",
        "with open(data_path, 'r') as f:\n",
        "  data = f.read()\n",
        "\n",
        "# получение всех строк \n",
        "lines = data.split('\\n')\n",
        "# получение заголовков всех столбцов\n",
        "header = lines[0].split(',')\n",
        "lines = lines[1:]\n",
        "\n",
        "float_data = np.zeros((len(lines), len(header) - 1))\n",
        "for i, line in enumerate(lines):\n",
        "  # каждое значение в строке отделено запятой\n",
        "  values = [float(x) for x in line.split(',')[1:]]\n",
        "  # все переведенные в float числа помещает в массив\n",
        "  float_data[i, :] = values\n",
        "\n",
        "# все данные измеряются в разных диапозонах (температура, давление...)\n",
        "# поэтому нормализуем их\n",
        "mean = float_data[:200_000].mean(axis=0)\n",
        "float_data -= mean\n",
        "std = float_data[:200_000].std(axis=0)\n",
        "float_data /= std\n",
        "\n",
        "# принимает: текущий массив данных\n",
        "# возвращает: пакеты, представляющие собой недавнее прошлое, предсказанную температуру\n",
        "# data - нормализованные данные\n",
        "# lookback - количество интервалов из прошлого\n",
        "# delay - количество интервалов из будущего\n",
        "# min_index, max_index - границы в data, в которых выбираются значения\n",
        "# shuffle - извлекать по порядку или с перемешиванием\n",
        "# batch_size - количество образцов в пакете\n",
        "# step - период в интервалах, в котором выбираются значения (6 - значит каждый час)\n",
        "\n",
        "def generator(data, lookback, delay, min_index, max_index, \n",
        "              shuffle=False, batch_size=128, step=6):\n",
        "  # если правая граница не указана - она определяется сама\n",
        "  if max_index is None:\n",
        "    max_index = len(data) - delay - 1\n",
        "  # правая граница первого интервала\n",
        "  i = min_index + lookback\n",
        "  while True:\n",
        "    if shuffle:\n",
        "      # если данные не извлекаются по порядку, то генерируем список случайных\n",
        "      # строк, откуда эти данные будем брать\n",
        "      rows = np.random.randint(min_index + lookback, max_index, size=batch_size)\n",
        "    else:\n",
        "      if i + batch_size > max_index:\n",
        "        # если дошли до конца (до самой правой границы), то возвращаемся в начало\n",
        "        i = min_index + lookback\n",
        "      # генерируем список подряд идущиъ индексов строк\n",
        "      rows = np.arange(i, min(i + batch_size, max_index))\n",
        "      # смещаем правую границу первого интервала к концу только что созданного списка\n",
        "      # так как мы его будем обрабатывать и больше его начало нас не интересует\n",
        "      i += len(rows)\n",
        "    # создаем тензоры образцов и целей\n",
        "    # все это - просто часть известных данных из файла\n",
        "    # samples - данные из прошлого\n",
        "    # targets - данные (как бы) из будущего (просто правее)\n",
        "    samples = np.zeros((len(rows), lookback // step, data.shape[-1]))\n",
        "    targets = np.zeros((len(rows),))\n",
        "    for j, row in enumerate(rows):\n",
        "      # берем lookback значений из прошлого с интервалом step\n",
        "      indices = range(rows[j] - lookback, rows[j], step)\n",
        "      # извлекаем образцы из прошлого по индексам\n",
        "      samples[j] = data[indices]\n",
        "      # извлекаем цели (которые тоже образцы) из будущего\n",
        "      targets[j] = data[rows[j] + delay][1]\n",
        "    # образцы возвращаются в прямом порядке\n",
        "    # для большинства временных последовательностей если сеть обрабатывает последовательность в прямом порядке\n",
        "    # то она лучше обучается, чем если в обратном\n",
        "    yield samples, targets    \n",
        "\n",
        "# 100 дней назад\n",
        "lookback = 14400\n",
        "# замеры раз в полчаса\n",
        "step = 3\n",
        "# вперед на день\n",
        "delay = 144\n",
        "batch_size = 128\n",
        "\n",
        "train_generator = generator(\n",
        "    float_data, \n",
        "    lookback=lookback,\n",
        "    delay=delay,\n",
        "    min_index=0,\n",
        "    max_index=200_000,\n",
        "    shuffle=True,\n",
        "    step=step,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "val_generator = generator(\n",
        "    float_data, \n",
        "    lookback=lookback,\n",
        "    delay=delay,\n",
        "    min_index=200_001,\n",
        "    max_index=300_000,\n",
        "    shuffle=True,\n",
        "    step=step,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "test_generator = generator(\n",
        "    float_data, \n",
        "    lookback=lookback,\n",
        "    delay=delay,\n",
        "    min_index=300_000,\n",
        "    max_index=None,\n",
        "    shuffle=True,\n",
        "    step=step,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "# сколько раз надо обратиться к val_generator, чтобы получить проверочный набор целиком\n",
        "val_steps = (300_000 - 200_001 - lookback) // batch_size\n",
        "# сколько раз надо обратиться к test_generator, чтобы получить тестовый набор целиком\n",
        "test_steps = (len(float_data) - 300_001 - lookback) // batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkTG0z0eJHN6"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv1D, MaxPooling1D, GlobalMaxPooling1D, GRU\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv1D(32, 5, activation='relu', input_shape=(None, float_data.shape[-1])))\n",
        "model.add(MaxPooling1D(3))\n",
        "model.add(Conv1D(32, 5, activation='relu'))\n",
        "model.add(GRU(32, dropout=0.1))\n",
        "model.add(Dense(1))\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='mae')\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=500,\n",
        "    epochs=20,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=val_steps\n",
        ").history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUd4OjjyKGpA"
      },
      "source": [
        "def draw_graph(history):\n",
        "  import matplotlib.pyplot as plt\n",
        "  loss = history['loss']\n",
        "  val_loss = history['val_loss']\n",
        "  epochs = range(1, len(loss) + 1)\n",
        "  plt.figure()\n",
        "  plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "  plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "  plt.title('Training and validation loss')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "draw_graph(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiOi4Iw9Kc4u"
      },
      "source": [
        "Такая архитектура не дотягивает до решения с регуляризированным GRU, однако он действует намного быстрее"
      ]
    }
  ]
}